# Python Data Ingestion to AWS S3

## ğŸ“Œ VisÃ£o Geral
Este projeto demonstra um pipeline simples de **ingestÃ£o de dados**, desenvolvido com foco em **Engenharia de Dados**.

O pipeline realiza:
- Consumo de dados a partir de uma **API pÃºblica**
- TransformaÃ§Ã£o bÃ¡sica utilizando **Pandas**
- PersistÃªncia local em formato **CSV**
- Upload dos dados para a **AWS S3**, organizados em uma estrutura de Data Lake

O objetivo Ã© demonstrar conceitos fundamentais de ingestÃ£o, organizaÃ§Ã£o de dados e integraÃ§Ã£o com serviÃ§os cloud.

---

## ğŸ§± Arquitetura do Pipeline

API PÃºblica  
â†“  
Python (requests + pandas)  
â†“  
CSV (local)  
â†“  
AWS S3 (camada raw)


## ğŸ§± Os dados sÃ£o armazenados no S3 seguindo o padrÃ£o:
s3://<bucket-name>/raw/users/dt=YYYY-MM-DD/users_YYYYMMDD_HHMMSS.csv



---

## ğŸ› ï¸ Tecnologias Utilizadas
- Python 3.10+
- Pandas
- Requests
- Boto3
- AWS S3
- AWS IAM
- Git / GitHub

---

## ğŸ“‚ Estrutura do Projeto

â”œâ”€â”€ src/  
â”‚ â”œâ”€â”€ ingest.py  
â”‚ â””â”€â”€ config.py  
â”œâ”€â”€ data/  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md


---

## âš™ï¸ PrÃ©-requisitos
- Python 3.10 ou superior
- Conta AWS
- AWS CLI configurado (`aws configure`)
- PermissÃµes para escrita em bucket S3

---

## ğŸš€ Como Executar o Projeto

### 1 Clonar o repositÃ³rio
```bash
git clone https://github.com/al-ma-ab/python-data-ingestion.git
cd python-data-ingestion
```
### 2 Clonar o repositÃ³rio
```bash
python -m venv .venv
source .venv/bin/activate
```

### 3 Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4 Configurar o bucket S3
-- Edite o arquivo src/config.py
```bash
S3_BUCKET = "nome-do-seu-bucket"
```
### 5 Executar ingestÃ£o
```bash
python -m src.ingest
```
### âœ… Resultado Esperado

- Um arquivo CSV serÃ¡ criado localmente no diretÃ³rio data/  
- O mesmo arquivo serÃ¡ enviado para o bucket S3 na camada raw/  
- O caminho completo do arquivo serÃ¡ exibido no terminal apÃ³s a execuÃ§Ã£o


### ğŸ”’ SeguranÃ§a

- As credenciais AWS nÃ£o estÃ£o versionadas
- O acesso Ã  AWS Ã© realizado via AWS CLI e IAM
- Nenhuma chave sensÃ­vel Ã© armazenada no cÃ³digo

### ğŸ¯ PrÃ³ximos Passos

- Evoluir o pipeline para salvar dados em formato Parquet
- Introduzir logging estruturado
- Implementar tratamento de falhas e retries
- Processar os dados utilizando PySpark
- Criar camada processed no Data Lake


## ğŸ‘¤ Autor

Alexandre Martins  
Engenheiro de Dados
